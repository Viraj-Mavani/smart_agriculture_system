{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T17:01:09.958511Z",
     "start_time": "2024-12-19T17:01:02.537664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.src.applications.efficientnet import EfficientNetB0\n",
    "from keras.src.legacy.preprocessing.image import NumpyArrayIterator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50, MobileNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score, precision_score, \\\n",
    "    recall_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T17:01:10.782464Z",
     "start_time": "2024-12-19T17:01:10.777660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Define paths\n",
    "data_dir = 'data'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "test_dir = os.path.join(data_dir, 'test')"
   ],
   "id": "101b6a8bfdf46fa3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T17:01:12.024596Z",
     "start_time": "2024-12-19T17:01:11.729610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Augmentation\n",
    "data_gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    ")\n",
    "\n",
    "# Train Data\n",
    "train_data = data_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    ")\n",
    "\n",
    "# Validation Data\n",
    "val_data = data_gen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    ")\n",
    "\n",
    "# Test Data\n",
    "test_data = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Calculate Class Weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_data.classes),\n",
    "    y=train_data.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n"
   ],
   "id": "ce15ffdc75c06cd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7000 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 3000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T17:01:13.850369Z",
     "start_time": "2024-12-19T17:01:13.645288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_batch, y_batch = next(iter(train_data))\n",
    "print(\"X batch shape:\", x_batch.shape)\n",
    "print(\"Y batch shape:\", y_batch.shape)\n",
    "print(\"Data type of X:\", x_batch.dtype)\n",
    "print(\"Data type of Y:\", y_batch.dtype)"
   ],
   "id": "91c597937c1cda7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (32, 224, 224, 3)\n",
      "Y batch shape: (32, 10)\n",
      "Data type of X: float32\n",
      "Data type of Y: float32\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-19T17:01:19.250574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define CNN Model\n",
    "cnn_model = Sequential([\n",
    "    tf.keras.layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Flatten(),\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_data.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('best_cnn_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Train CNN Model\n",
    "cnn_history = cnn_model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS,\n",
    "    # callbacks=[early_stopping, lr_scheduler],\n",
    "    callbacks=[early_stopping, lr_scheduler, model_checkpoint],\n",
    "    # class_weight=class_weights\n",
    ")"
   ],
   "id": "ea85f329b7262b96",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viraj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m116/219\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m1:34\u001B[0m 920ms/step - accuracy: 0.4309 - loss: 3.2429"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fine-Tuned ResNet50 Model\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "resnet_base.trainable = True  # Fine-tune\n",
    "\n",
    "resnet_model = Sequential([\n",
    "    resnet_base,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_data.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "resnet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train ResNet50 Model\n",
    "resnet_history = resnet_model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, lr_scheduler],\n",
    "    # class_weight=class_weights\n",
    ")"
   ],
   "id": "e0ce1fd8ee6738ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # New EfficientNet Model\n",
    "# eff_model = Sequential([\n",
    "#     EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "#     GlobalAveragePooling2D(),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(train_data.num_classes, activation='softmax')\n",
    "# ])\n",
    "# \n",
    "# eff_model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# \n",
    "# # Train EfficientNet Model\n",
    "# eff_history = eff_model.fit(\n",
    "#     train_data,\n",
    "#     validation_data=val_data,\n",
    "#     epochs=EPOCHS,\n",
    "#     callbacks=[early_stopping, lr_scheduler],\n",
    "#     class_weight=class_weights\n",
    "# )"
   ],
   "id": "56cd5e730226eb5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save Models\n",
    "cnn_model.save('cnn_model.keras')\n",
    "resnet_model.save('resnet50_model.keras')"
   ],
   "id": "f5b3c3b04e1d47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Model Function\n",
    "def evaluate_model(model, data, model_name=\"Model\"):\n",
    "    loss, accuracy = model.evaluate(data)\n",
    "    print(f\"{model_name} Test Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Generate Predictions\n",
    "    data.reset()\n",
    "    predictions = model.predict(data)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = data.classes\n",
    "\n",
    "    # Classification Metrics\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=list(data.class_indices.keys())))\n",
    "\n",
    "    # Compute Evaluation Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')  # Weighted for imbalanced classes\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_true, predictions, multi_class='ovr')  # For multi-class problems\n",
    "    \n",
    "    # Print Metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(data.class_indices.keys()),\n",
    "                yticklabels=list(data.class_indices.keys()))\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Evaluate All Models\n",
    "evaluate_model(cnn_model, test_data, \"CNN\")\n",
    "evaluate_model(resnet_model, test_data, \"ResNet50\")\n",
    "# evaluate_model(eff_model, val_data, \"EfficientNetB0\")"
   ],
   "id": "48841b71463ca15e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tr_plot(tr_data, start_epoch):\n",
    "    # Extract metrics from training history\n",
    "    train_acc = tr_data.history['accuracy']\n",
    "    train_loss = tr_data.history['loss']\n",
    "    val_acc = tr_data.history['val_accuracy']\n",
    "    val_loss = tr_data.history['val_loss']\n",
    "\n",
    "    # Calculate epoch count and epoch range\n",
    "    total_epochs = len(train_acc) + start_epoch\n",
    "    epochs = list(range(start_epoch + 1, total_epochs + 1))\n",
    "\n",
    "    # Identify the best epochs based on validation loss and accuracy\n",
    "    best_val_loss_epoch = np.argmin(val_loss)  # Epoch with the lowest validation loss\n",
    "    best_val_loss = val_loss[best_val_loss_epoch]\n",
    "\n",
    "    best_val_acc_epoch = np.argmax(val_acc)  # Epoch with the highest validation accuracy\n",
    "    best_val_acc = val_acc[best_val_acc_epoch]\n",
    "\n",
    "    # Plot style\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    # Labels for best epochs\n",
    "    loss_label = f\"Best Epoch (Loss): {best_val_loss_epoch + 1 + start_epoch}\"\n",
    "    acc_label = f\"Best Epoch (Accuracy): {best_val_acc_epoch + 1 + start_epoch}\"\n",
    "\n",
    "    # Create subplots for loss and accuracy\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    axes[0].plot(epochs, train_loss, 'r', label='Training Loss')\n",
    "    axes[0].plot(epochs, val_loss, 'g', label='Validation Loss')\n",
    "    axes[0].scatter(best_val_loss_epoch + 1 + start_epoch, best_val_loss, s=150, c='blue', label=loss_label)\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    axes[1].plot(epochs, train_acc, 'r', label='Training Accuracy')\n",
    "    axes[1].plot(epochs, val_acc, 'g', label='Validation Accuracy')\n",
    "    axes[1].scatter(best_val_acc_epoch + 1 + start_epoch, best_val_acc, s=150, c='blue', label=acc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the correct parameters\n",
    "tr_plot(cnn_history, 0)\n",
    "tr_plot(resnet_history, 0)"
   ],
   "id": "974fea913bcef9d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fe79663af03580a6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
